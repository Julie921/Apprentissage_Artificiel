{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30683d5",
   "metadata": {},
   "source": [
    "# Looking for features\n",
    "\n",
    "- La fréquence d'utilisation de certains mots ou expressions clés associés à chaque parti politique.\n",
    "- La longueur des phrases et la complexité du vocabulaire utilisé.\n",
    "- La fréquence d'utilisation de certaines émotions ou sentiments, tels que la colère, la peur, l'espoir, etc.\n",
    "- La fréquence d'utilisation de certaines structures syntaxiques ou grammaticales caractéristiques de chaque parti politique.\n",
    "- La fréquence d'utilisation de certaines références culturelles ou historiques associées à chaque parti politique.\n",
    "- La fréquence d'utilisation de certaines sources d'information ou de données factuelles, comme les chiffres ou les statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0949e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### modules pour le chargement des données depuis le XML ######\n",
    "import glob\n",
    "from lxml import etree\n",
    "from preTraitements.xml import get_X_Y_from_root\n",
    "from preTraitements.xml import get_tree_root_from_file\n",
    "\n",
    "###### modules pour la classification ######\n",
    "\n",
    "# modèles\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# création de nos transformers\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer # créer nos propres transformer\n",
    "\n",
    "# recherche des meilleurs hyperparamètres\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# résultats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# sauvegarde des modèles\n",
    "from joblib import dump, load\n",
    "\n",
    "###### modules pour la visualisation ######\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "###### miscellaneous ######\n",
    "from typing import List # typage des fonctions\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "##### multi class #####\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe1d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e587943",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train, root_train = get_tree_root_from_file(\"./corpus/train_deft09_parlement_appr.xml/deft09_parlement_appr_fr.xml\")\n",
    "X_train, y_train = get_X_Y_from_root(root_train)\n",
    "\n",
    "tree_test, root_test = get_tree_root_from_file(\"./corpus/deft09_parlement_test.xml/deft09_parlement_test_fr.xml\")\n",
    "X_test, y_test = get_X_Y_from_root(root_test) # y_test est vide : pas accès aux résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e46707e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_clean = re.compile(r\"[^ \\w]\") # pattern à utiliser pour nettoyer les données\n",
    "\n",
    "def clean(data): # TODO: améliorer la fonction\n",
    "    \"\"\"TODO: écrire la docstring\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    global pattern_clean\n",
    "    return re.sub(pattern_clean, \"\", data).lower()\n",
    "\n",
    "X_train_clean = [clean(x) for x in X_train]\n",
    "X_test_clean = [clean(x) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c5ec14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\d+\\t(\\w+(-\\w+)?)')\n",
    "y_test = []\n",
    "with open(\"./corpus/deft09_parlement_ref/deft09_parlement_ref_fr.txt\",'r') as file:\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        m= re.match(pattern,line)\n",
    "        if m:\n",
    "            y_test.append(m.group(1))\n",
    "        else:\n",
    "            y_test.append('PSE')\n",
    "        line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "360b0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Créer un objet LabelBinarizer\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Convertir les étiquettes de classe en un tableau binaire\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3554f5",
   "metadata": {},
   "source": [
    "# Les mots les plus fréquents par partis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc165e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat = list(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c047ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Verts-ALE', 'PPE-DE', 'Verts-ALE', 'PSE', 'PSE', 'Verts-ALE', 'PPE-DE', 'PPE-DE', 'PSE', 'PPE-DE', 'PSE', 'PPE-DE', 'ELDR', 'GUE-NGL', 'Verts-ALE', 'PSE', 'GUE-NGL', 'PSE', 'PSE', 'PSE']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b05e2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376 5440 6858 2008 2688\n"
     ]
    }
   ],
   "source": [
    "vert = []\n",
    "pse = []\n",
    "ppe = []\n",
    "eldr= []\n",
    "gue = []\n",
    "\n",
    "for element in resultat:\n",
    "    if element[1] == \"Verts-ALE\":\n",
    "        vert.append(element)\n",
    "    if element[1] == \"PPE-DE\":\n",
    "        ppe.append(element)\n",
    "    if element[1] == \"PSE\":\n",
    "        pse.append(element)\n",
    "    if element[1] == \"ELDR\":\n",
    "        eldr.append(element)\n",
    "    if element[1] == \"GUE-NGL\":\n",
    "            gue.append(element)\n",
    "            \n",
    "print(len(vert),len(pse),len(ppe),len(eldr),len(gue))\n",
    "\n",
    "total_examples = 2376 + 5440 + 6858 + 2008 + 2688  # Nombre total d'exemples dans les données d'entraînement\n",
    "\n",
    "weight_0 = total_examples / 2376  # Poids de la classe 0\n",
    "weight_1 = total_examples / 5440  # Poids de la classe 1\n",
    "weight_2 = total_examples / 6858  # Poids de la classe 2\n",
    "weight_3 = total_examples / 2008  # Poids de la classe 3\n",
    "weight_4 = total_examples / 2688  # Poids de la classe 4\n",
    "\n",
    "class_weights = {0: weight_0, 1: weight_1, 2: weight_2, 3: weight_3, 4: weight_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array(['ELDR', 'GUE-NGL', 'PPE-DE', 'PSE', 'Verts-ALE'], dtype='<U9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24a0afa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 2688 6858 5440 2376\n"
     ]
    }
   ],
   "source": [
    "print(len(eldr),len(gue),len(ppe),len(pse),len(vert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d58c8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "total_examples = 2376 + 5440 + 6858 + 2008 + 2688  # Nombre total d'exemples dans les données d'entraînement\n",
    "\n",
    "weight_0 = total_examples / 2008  # Poids de la classe 0\n",
    "weight_1 = total_examples / 2688  # Poids de la classe 1\n",
    "weight_2 = total_examples / 6858  # Poids de la classe 2\n",
    "weight_3 = total_examples / 5440  # Poids de la classe 3\n",
    "weight_4 = total_examples / 2378  # Poids de la classe 4\n",
    "\n",
    "class_weights = {0: weight_0, 1: weight_1, 2: weight_2, 3: weight_3, 4: weight_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dad8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def most_frequent_words_by_label(phrases, labels):\n",
    "    # Créer un dictionnaire vide qui va stocker les mots les plus fréquents pour chaque parti\n",
    "    word_counts_by_label = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Créer une liste de stopwords en français\n",
    "    stopwords_list = stopwords.words('french')\n",
    "    stopwords_list.extend([\"être\", \"avoir\", \"faire\",\"’\",\"a\",\"commission\",'monsieur', 'président','parlement', 'rapport'])\n",
    "    \n",
    "    # Créer un lemmatiseur\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Pour chaque phrase et son label correspondant, compter les mots\n",
    "    for phrase, label in zip(phrases, labels):\n",
    "        # Tokeniser la phrase en mots\n",
    "        words = word_tokenize(phrase.lower())\n",
    "\n",
    "        # Pour chaque mot, incrémenter son compteur dans le dictionnaire de mots pour le label correspondant\n",
    "        for word in words:\n",
    "            # Si le mot n'est pas une stopword, n'est pas un verbe \"être\", \"avoir\" ou \"faire\" et n'est pas de la ponctuation, incrémenter son compteur\n",
    "            if word not in stopwords_list and word not in string.punctuation:\n",
    "                lemma = lemmatizer.lemmatize(word)\n",
    "                if lemma not in [\"être\",\"avoir\",\"faire\"]:\n",
    "                    word_counts_by_label[label][lemma] += 1\n",
    "\n",
    "    # Pour chaque parti, trouver les 10 mots les plus fréquents\n",
    "    most_frequent_words_by_label = {}\n",
    "    for label, word_counts in word_counts_by_label.items():\n",
    "        # Trouver les 10 mots les plus fréquents en triant le dictionnaire par compte de mot\n",
    "        most_frequent_words = sorted(word_counts, key=word_counts.get, reverse=True)[:10]\n",
    "        most_frequent_words_by_label[label] = most_frequent_words\n",
    "\n",
    "    return most_frequent_words_by_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a8629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [text for text,parti in resultat]\n",
    "labels = [parti for text,parti in resultat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab859381",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(most_frequent_words_by_label(phrases, labels))\n",
      "Cell \u001b[0;32mIn [10], line 21\u001b[0m, in \u001b[0;36mmost_frequent_words_by_label\u001b[0;34m(phrases, labels)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pour chaque phrase et son label correspondant, compter les mots\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(phrases, labels):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Tokeniser la phrase en mots\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Pour chaque mot, incrémenter son compteur dans le dictionnaire de mots pour le label correspondant\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Si le mot n'est pas une stopword, n'est pas un verbe \"être\", \"avoir\" ou \"faire\" et n'est pas de la ponctuation, incrémenter son compteur\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(most_frequent_words_by_label(phrases, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c7f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def most_frequent_words_by_label(phrases, labels):\n",
    "    # Charger la liste de stop words en français\n",
    "    stop_words = stopwords.words('french')\n",
    "    # Ajouter la ponctuation à la liste de stop words\n",
    "    stop_words += list(string.punctuation)\n",
    "\n",
    "    # Créer un dictionnaire vide qui va stocker les mots les plus fréquents pour chaque parti\n",
    "    word_counts_by_label = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Pour chaque phrase et son label correspondant, compter les mots\n",
    "    for phrase, label in zip(phrases, labels):\n",
    "        # Tokeniser la phrase en mots\n",
    "        words = word_tokenize(phrase.lower())\n",
    "\n",
    "        # Pour chaque mot, incrémenter son compteur dans le dictionnaire de mots pour le label correspondant\n",
    "        # mais ignorer les stop words et la ponctuation\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                word_counts_by_label[label][word] += 1\n",
    "\n",
    "    # Pour chaque parti, trouver les 10 mots les plus fréquents\n",
    "    most_frequent_words_by_label = {}\n",
    "    for label, word_counts in word_counts_by_label.items():\n",
    "        # Trouver les 10 mots les plus fréquents en triant le dictionnaire par compte de mot\n",
    "        most_frequent_words = sorted(word_counts, key=word_counts.get, reverse=False)[:10]\n",
    "        most_frequent_words_by_label[label] = most_frequent_words\n",
    "\n",
    "    return most_frequent_words_by_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49b8591",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(most_frequent_words_by_label(phrases, labels))\n",
      "Cell \u001b[0;32mIn [13], line 19\u001b[0m, in \u001b[0;36mmost_frequent_words_by_label\u001b[0;34m(phrases, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pour chaque phrase et son label correspondant, compter les mots\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(phrases, labels):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Tokeniser la phrase en mots\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Pour chaque mot, incrémenter son compteur dans le dictionnaire de mots pour le label correspondant\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# mais ignorer les stop words et la ponctuation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/nltk/tokenize/destructive.py:181\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    178\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 181\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    183\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m/usr/lib/python3.8/re.py:325\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    322\u001b[0m     template \u001b[38;5;241m=\u001b[39m sre_parse\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(most_frequent_words_by_label(phrases, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fbaff",
   "metadata": {},
   "source": [
    "# Complexité du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af12082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# return : liste de dictionnaire où chaque dictionnaire -> complexité du vocabulaire.\n",
    "def calculate_vocab_complexity(post):\n",
    "    liste_finale = []\n",
    "    # Créer un set pour stocker les mots uniques\n",
    "    unique_words = set()\n",
    "\n",
    "    # Pour chaque phrase, ajouter les mots uniques au set\n",
    "    for phrases in post:\n",
    "        vocab_complexity = {}\n",
    "        for phrase in phrases.split():\n",
    "            words = word_tokenize(phrase)\n",
    "            for word in words:\n",
    "                unique_words.add(word)\n",
    "\n",
    "        # Compter le nombre total de mots\n",
    "        total_word_count = 0\n",
    "        for phrase in phrases:\n",
    "            total_word_count += len(word_tokenize(phrase))\n",
    "\n",
    "        # Calculer la complexité du vocabulaire en divisant le nombre de mots uniques par le nombre total de mots\n",
    "        vocab_complexity[\"complexity\"] = len(unique_words) / total_word_count\n",
    "        liste_finale.append(vocab_complexity)\n",
    "    return liste_finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363bf7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'complexity': 0.3}, {'complexity': 0.3125}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_vocab_complexity([\"Julie est là\",\"Julie est aussi ici\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b9bb7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.335) total time= 1.1min\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.335) total time= 1.3min\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.349) total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Ajoutez les prédictions de thèmes à votre pipeline de classification de discours politiques\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pipeline_avec_complex = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('vec', Pipeline([\n",
    "            ('count', CountVectorizer(min_df=2, max_df=0.8)),\n",
    "            ('tf', TfidfTransformer())\n",
    "        ])),\n",
    "        ('theme', Pipeline([\n",
    "          ('stats', FunctionTransformer(calculate_vocab_complexity)),\n",
    "          ('vect', DictVectorizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    #('pca', PCA()),  # Ajout de la réduction de la dimensionalité par ACP ici\n",
    "    ('standard', StandardScaler(with_mean=False)),\n",
    "    ('rForest', RandomForestClassifier(criterion='gini',max_features='sqrt'))\n",
    "])\n",
    "\n",
    "# Entraînez le modèle de classification de discours politiques sur les données d'entraînement\n",
    "#pipeline_avec_theme.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "cv_avec_complex = cross_val_score(pipeline_avec_complex, X_train_clean[:500], y_train_bin[:500], verbose=10, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be654e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.04        52\n",
      "           1       0.62      0.07      0.12        73\n",
      "           2       0.39      0.39      0.39       177\n",
      "           3       0.27      0.62      0.38       128\n",
      "           4       0.16      0.04      0.07        70\n",
      "\n",
      "    accuracy                           0.31       500\n",
      "   macro avg       0.49      0.23      0.20       500\n",
      "weighted avg       0.42      0.31      0.26       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_avec_complex.fit(X_train_clean[:100],y_train_bin[:100])\n",
    "y_pred = pipeline_avec_complex.predict(X_test_clean[:500])\n",
    "print(classification_report(y_test_bin[:500],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a7e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334f1067",
   "metadata": {},
   "source": [
    "# Les émotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c43b4",
   "metadata": {},
   "source": [
    "Il existe plusieurs lexiques de sentiments pour le français. Vous pouvez par exemple utiliser le lexique \"LIWC\" (Linguistic Inquiry and Word Count), qui contient plus de 6 000 mots et expressions en français associés à des émotions et sentiments. Vous pouvez également utiliser le lexique \"AFINN\", qui contient environ 2 500 mots et expressions en français associés à des émotions et sentiments, ainsi que des scores de polarité positive et négative. Vous pouvez également utiliser le lexique \"NRC Emotion Lexicon\", qui contient environ 14 000 mots et expressions en français associés à huit émotions différentes (joie, peur, colère, tristesse, surprise, dégoût, confiance et anticipation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae42759",
   "metadata": {},
   "source": [
    "# Les structures grammaticales et syntaxiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5f939c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def count_4grams(phrases):\n",
    "    for phrase in phrases:\n",
    "        # Tokeniser la phrase en mots\n",
    "        words = word_tokenize(phrase)\n",
    "\n",
    "        # Générer les 4-grams de la phrase\n",
    "        four_grams = ngrams(words, 4)\n",
    "\n",
    "        # Compter les 4-grams\n",
    "        four_gram_counts = defaultdict(int)\n",
    "        for four_gram in four_grams:\n",
    "            four_gram_counts[four_gram] += 1\n",
    "    \n",
    "    return sorted(four_gram_counts, key=four_gram_counts.get, reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "75e06bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Monsieur', 'le', 'Président', ','),\n",
       " ('le', 'Président', ',', 'le'),\n",
       " ('Président', ',', 'le', 'rapport'),\n",
       " (',', 'le', 'rapport', 'de'),\n",
       " ('le', 'rapport', 'de', 'Mme'),\n",
       " ('rapport', 'de', 'Mme', 'Zorba'),\n",
       " ('de', 'Mme', 'Zorba', 'est'),\n",
       " ('Mme', 'Zorba', 'est', 'très'),\n",
       " ('Zorba', 'est', 'très', 'intelligent'),\n",
       " ('est', 'très', 'intelligent', ',')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_4grams(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0f19ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def most_frequent_pos_4grams(phrase):\n",
    "    # Tokeniser la phrase en mots\n",
    "    for phrase in phrases:\n",
    "        words = word_tokenize(phrase)\n",
    "\n",
    "        # Tagger les mots avec leur partie du discours\n",
    "        pos_tags = pos_tag(words)\n",
    "\n",
    "        # Créer un dictionnaire vide qui va stocker les 4-grams de POS les plus fréquents\n",
    "        pos_4gram_counts = defaultdict(int)\n",
    "\n",
    "        # Pour chaque 4-gram de POS, incrémenter son compteur dans le dictionnaire\n",
    "        for i in range(len(pos_tags) - 3):\n",
    "            pos_4gram = pos_tags[i][1] + \" \" + pos_tags[i+1][1] + \" \" + pos_tags[i+2][1] + \" \" + pos_tags[i+3][1]\n",
    "            pos_4gram_counts[pos_4gram] += 1\n",
    "\n",
    "        # Trouver les 10 4-grams de POS les plus fréquents en triant le dictionnaire par compte de 4-gram\n",
    "        most_frequent_pos_4grams = sorted(pos_4gram_counts, key=pos_4gram_counts.get, reverse=True)[:10]\n",
    "\n",
    "    return most_frequent_pos_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdcccd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JJ NN NN NN',\n",
       " 'NN NN JJ NN',\n",
       " 'NN JJ NN NN',\n",
       " 'JJ NN IN NN',\n",
       " 'FW FW NN FW',\n",
       " 'NN NN NN NN',\n",
       " 'FW FW FW NN',\n",
       " 'NNS VBP JJ NN',\n",
       " 'NN NN , JJ',\n",
       " 'JJ NN NN JJ']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_pos_4grams(phrases[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56848efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
