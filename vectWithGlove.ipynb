{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import spacy\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation avec Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: List[str], nlp) -> List[List[str]]:\n",
    "    \"\"\"Fonction qui, à partir d'une liste de string, renvoie une liste de listes de string\n",
    "    Chaque sous-liste correspond aux tokens d'un élément de la liste de départ\n",
    "    La tokenization ne prend pas en compte les stop words, la poncutation et les espaces.\n",
    "\n",
    "    Args:\n",
    "        text (List[str]): une liste de string\n",
    "        nlp (spacy.lang.fr.French): un modèle de spacy\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]] une liste de listes de string, chaque élément correspond à un token\n",
    "    \"\"\"\n",
    "    return [[token.text for token in nlp(enonce) if not (token.is_stop or token.is_punct or token.is_space)] for enonce in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\") # chargement du modèle spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['veux', 'fonction', 'écrite'], ['teste', 'choses'], ['teste']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"je veux une fonction que j'ai écrite\", \"je teste des choses\", \"je teste\"]\n",
    "tokenize(a, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unable to handle scheme 'c', expected one of ('', 'file', 'ftp', 'ftps', 'hdfs', 'http', 'https', 'viewfs', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m unzipped_file \u001b[39m=\u001b[39m get_tmpfile(\u001b[39m\"\u001b[39m\u001b[39mglove.6B.100d.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Load the GloVe model into a KeyedVectors object\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(zip_file)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Access the word vectors using the model's word_vec method\u001b[39;00m\n\u001b[0;32m     25\u001b[0m vector \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mword_vec(\u001b[39m\"\u001b[39m\u001b[39mbonjour\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aengp\\Documents\\SCHOOL\\M1_TAL\\s9\\apprentissage_artificiel\\tp\\.venvML\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aengp\\Documents\\SCHOOL\\M1_TAL\\s9\\apprentissage_artificiel\\tp\\.venvML\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\aengp\\Documents\\SCHOOL\\M1_TAL\\s9\\apprentissage_artificiel\\tp\\.venvML\\lib\\site-packages\\smart_open\\smart_open_lib.py:224\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m ve:\n\u001b[0;32m    222\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(ve\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 224\u001b[0m binary \u001b[39m=\u001b[39m _open_binary_stream(uri, binary_mode, transport_params)\n\u001b[0;32m    225\u001b[0m decompressed \u001b[39m=\u001b[39m so_compression\u001b[39m.\u001b[39mcompression_wrapper(binary, binary_mode, compression)\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mor\u001b[39;00m explicit_encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aengp\\Documents\\SCHOOL\\M1_TAL\\s9\\apprentissage_artificiel\\tp\\.venvML\\lib\\site-packages\\smart_open\\smart_open_lib.py:399\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, transport_params)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt know how to handle uri \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mrepr\u001b[39m(uri))\n\u001b[0;32m    398\u001b[0m scheme \u001b[39m=\u001b[39m _sniff_scheme(uri)\n\u001b[1;32m--> 399\u001b[0m submodule \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mget_transport(scheme)\n\u001b[0;32m    400\u001b[0m fobj \u001b[39m=\u001b[39m submodule\u001b[39m.\u001b[39mopen_uri(uri, mode, transport_params)\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(fobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aengp\\Documents\\SCHOOL\\M1_TAL\\s9\\apprentissage_artificiel\\tp\\.venvML\\lib\\site-packages\\smart_open\\transport.py:95\u001b[0m, in \u001b[0;36mget_transport\u001b[1;34m(scheme)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m scheme \u001b[39min\u001b[39;00m _REGISTRY:\n\u001b[0;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m _REGISTRY[scheme]\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(message)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unable to handle scheme 'c', expected one of ('', 'file', 'ftp', 'ftps', 'hdfs', 'http', 'https', 'viewfs', 'webhdfs'). Extra dependencies required by 'c' may be missing. See <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst> for details."
     ]
    }
   ],
   "source": [
    "# # Load the pre-trained GloVe model\n",
    "# wv = Word2Vec.load('glove.model')\n",
    "\n",
    "# # Get the word embedding for the word \"cat\"\n",
    "# vector = wv['cat']\n",
    "\n",
    "# # Print the word embedding\n",
    "# print(vector)\n",
    "\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Specify the URL of the pre-trained GloVe model\n",
    "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "# Download and unzip the file\n",
    "zip_file = datapath(url)\n",
    "unzipped_file = get_tmpfile(\"glove.6B.100d.txt\")\n",
    "\n",
    "# Load the GloVe model into a KeyedVectors object\n",
    "model = KeyedVectors.load_word2vec_format(zip_file)\n",
    "\n",
    "# Access the word vectors using the model's word_vec method\n",
    "vector = model.word_vec(\"bonjour\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7772623b82ed846feb46f5a96aa7fb44349f73638c006ae7e8cad2b5cbf66b21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
