{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "506ac7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "35f8da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from lxml import etree\n",
    "from preTraitements.xml import get_X_Y_from_root\n",
    "from preTraitements.xml import get_tree_root_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb963fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0f1eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ab358f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "vector = {}\n",
    "pattern = re.compile(r\"(\\w+).+\")\n",
    "with open(\"vectors.txt\",'r',encoding=\"utf-8\") as file:\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        m=re.match(pattern,line)\n",
    "        #print(line[len(m.group(1)):].split())\n",
    "        if m:\n",
    "            vector[m.group(1)]=line[len(m.group(1)):].split()\n",
    "        line = file.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43c57d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train, root_train = get_tree_root_from_file(\"./corpus/train_deft09_parlement_appr.xml/deft09_parlement_appr_fr.xml\")\n",
    "X_train, y_train = get_X_Y_from_root(root_train)\n",
    "\n",
    "tree_test, root_test = get_tree_root_from_file(\"./corpus/deft09_parlement_test.xml/deft09_parlement_test_fr.xml\")\n",
    "X_test, y_test = get_X_Y_from_root(root_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c71c14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = X_train[:500]\n",
    "y_train_sample = y_train[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "83d4f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "D = X_train_sample\n",
    "testy = v.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b9ba7e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(testy,y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c8d9b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 12624)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8922453a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "085437c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "#root folder\n",
    "root_folder='.'\n",
    "glove_filename='vectors.txt'\n",
    "\n",
    "# Variable for data directory\n",
    "glove_path = os.path.abspath(glove_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7ad23d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188/3652708010.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41153, 50)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We just need to run this code once, the function glove2word2vec saves the Glove embeddings in the word2vec format \n",
    "# that will be loaded in the next section\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_filename='vectors.txt'\n",
    "#glove_input_file = glove_filename\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove2word2vec(glove_path, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "91d840a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "02456cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madame:  [ 0.013859  0.451532 -0.487101  0.168849 -0.327894  0.146996 -0.224299\n",
      " -0.178038 -0.070914 -0.51272  -0.421458 -0.469794 -0.545544  0.139949\n",
      " -0.006503 -0.140751  0.333008 -0.048612  0.200209  0.376872  0.138722\n",
      " -0.271306  0.140596  0.33297   0.664384  0.201292 -0.005523 -0.746854\n",
      " -0.041821 -0.062128 -0.132737 -0.275511  0.495469  0.177609 -0.082183\n",
      " -0.180198  0.092252  0.044848  0.353179  0.449488 -0.322928 -0.258066\n",
      " -0.727073  0.10923   0.008607  0.232664  0.044286  0.068772  0.311111\n",
      "  0.15182 ]\n"
     ]
    }
   ],
   "source": [
    "#Show a word embedding\n",
    "print('Madame: ',glove.get_vector('madame'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0b6f94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/41153 is de\n",
      "word #1/41153 is la\n",
      "word #2/41153 is et\n",
      "word #3/41153 is le\n",
      "word #4/41153 is à\n",
      "word #5/41153 is les\n",
      "word #6/41153 is des\n",
      "word #7/41153 is que\n",
      "word #8/41153 is en\n",
      "word #9/41153 is du\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(glove.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(glove.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8f7fb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(model.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "33d5d535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.756286,  0.357416,  0.110608,  0.386516,  1.550777,  1.69021 ,\n",
       "       -0.042318, -1.408475,  1.903917,  0.546989,  0.639621,  0.19015 ,\n",
       "        0.004072,  0.352885,  0.186876,  0.301915, -1.090066, -0.191985,\n",
       "        0.141774,  0.872651,  0.310097,  1.655799, -0.424185,  0.929607,\n",
       "       -0.435726,  0.742252,  0.304116,  1.093469, -0.420415, -0.042222,\n",
       "        1.265649,  2.294702, -0.523597, -0.644814,  0.603675, -0.493675,\n",
       "       -0.776827,  0.226664,  0.709328, -1.273558,  0.6168  ,  0.541127,\n",
       "        0.451895, -0.272934, -1.010776,  0.852358,  0.003264,  0.216668,\n",
       "       -1.258991,  0.387364], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9433366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = np.array([np.array([model[i] for i in ls if i in words])\n",
    "                         for ls in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5ce56709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GloveVectorizer(model,X_train):\n",
    "    return np.array([np.array([model[i] for i in ls if i in words])\n",
    "                         for ls in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a1bd01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188/1248266255.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([np.array([model[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "X_vect = GloveVectorizer(model,X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a507a1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5b6e9949",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [193], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m LinearSVC()\n\u001b[0;32m----> 3\u001b[0m Xtest \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mfit(X_vect,y_train_sample)\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/sklearn/svm/_classes.py:246\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPenalty term must be positive; got (C=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC)\n\u001b[0;32m--> 246\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m check_classification_targets(y)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/mnt/c/Users/Julie/OneDrive/Bureau/M2_TAL/apprentissage-artificiel/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = LinearSVC()\n",
    "Xtest = clf.fit(X_vect,y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2a8adce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La [-0.356746 -0.763595 -0.439038  0.400101  0.348126  0.664703  0.691902\n",
      " -1.370866  1.874059  0.008522  1.051088 -0.168902 -0.985847  0.559935\n",
      " -0.119273  0.312639 -1.364021 -0.140409 -0.977615  1.41729   0.493159\n",
      "  1.043272  0.192097 -0.421588  0.778566  0.823617 -0.90677   1.003853\n",
      "  0.504122 -0.020136  0.480741  1.802187 -0.216162 -0.827201 -0.15117\n",
      "  0.573967 -0.299327 -0.202961  0.406103 -0.561666  0.847018  0.813715\n",
      " -0.520787  0.008835 -0.471773  0.828857 -0.236493  0.549296 -1.436775\n",
      " -0.271421]\n",
      "Présidente [-2.985920e-01 -5.042320e-01 -2.069940e-01 -3.398690e-01  4.104580e-01\n",
      "  2.722290e-01  4.177560e-01 -5.758040e-01  2.407050e-01  4.824950e-01\n",
      " -3.483500e-02  9.440000e-04 -1.034850e+00 -9.299000e-03 -8.051290e-01\n",
      " -6.766020e-01 -1.368750e-01 -7.404110e-01  8.883000e-02  3.501070e-01\n",
      "  1.051480e-01  1.635700e-01  1.031557e+00  1.145090e-01 -2.906010e-01\n",
      " -2.816790e-01 -5.765150e-01 -5.500540e-01 -4.253330e-01 -3.232340e-01\n",
      " -3.044530e-01  1.001771e+00  1.047254e+00  2.982190e-01  7.651920e-01\n",
      "  2.356460e-01  4.020800e-02  3.178100e-01  1.392315e+00 -4.344750e-01\n",
      " -3.427430e-01 -6.981330e-01 -5.106840e-01  9.144890e-01  5.792520e-01\n",
      "  4.176770e-01  4.731880e-01 -9.102160e-01 -1.168566e+00 -4.828000e-02]\n",
      "parle. [-0.417953  0.150543  0.616063 -0.67447  -0.085915  0.344126  0.818555\n",
      " -0.417155 -0.839968  0.571792 -0.021682 -0.212905 -0.144317  0.842856\n",
      "  0.154145 -0.05403  -0.579552  0.24674  -0.329061 -0.226725 -0.357523\n",
      " -0.205394  0.275619 -0.11104  -0.537745 -0.024239 -0.665708 -0.888624\n",
      " -0.064363  0.092094  0.375153 -0.426794 -0.203939 -0.104164  0.531734\n",
      " -0.155817  0.16224   0.153594  0.206196  0.200626 -0.385353  0.306611\n",
      "  0.195391  0.649483 -0.071085  0.023498  0.307329 -0.053103  0.282941\n",
      " -0.024446]\n",
      "[[-0.35776368 -0.37242803 -0.00998966 -0.20474601  0.22422302  0.42701933\n",
      "   0.64273769 -0.78794163  0.424932    0.35426965  0.33152366 -0.12695433\n",
      "  -0.72167134  0.46449733 -0.25675234 -0.139331   -0.69348264 -0.21135999\n",
      "  -0.40594867  0.51355731  0.08026134  0.33381602  0.49975765 -0.139373\n",
      "  -0.01659333  0.17256631 -0.71633101 -0.14494169  0.00480867 -0.08375866\n",
      "   0.18381368  0.79238796  0.209051   -0.21104868  0.38191867  0.21793199\n",
      "  -0.03229299  0.089481    0.66820472 -0.26517168  0.03964065  0.14073099\n",
      "  -0.27869335  0.52426904  0.01213134  0.42334402  0.18134134 -0.13800766\n",
      "  -0.77413327 -0.11471567]]\n"
     ]
    }
   ],
   "source": [
    "data = [\"La Présidente parle.\"]\n",
    "n=0\n",
    "m = glove.get_vector('président')\n",
    "X = np.zeros((len(data),m.shape[0]))\n",
    "\n",
    "for phrase in data:\n",
    "    tokens = phrase.split()\n",
    "    vecs = []\n",
    "    for token in tokens:\n",
    "        v = glove.get_vector(token)\n",
    "        print(token,v)\n",
    "        vecs.append(v)\n",
    "    vecs = np.array(vecs)\n",
    "    X[n] = vecs.mean(axis=0)\n",
    "    n=n+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "42d703d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('président')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2b937a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9ccf3c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of samples with no words found: 0 / 500\n"
     ]
    }
   ],
   "source": [
    "# Get the sentence embeddings for the train dataset\n",
    "X_train = vectorizer.fit_transform(X_train_sample)\n",
    "y_train = y_train_sample\n",
    "# Get the sentence embeddings for the test dataset\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "#Y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffcb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53620373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0546f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b7736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19396516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de55325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('président')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e44939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monsieur le Président, j'ai toujours fait preuve d'un certain scepticisme quant aux comportements moralistes et policiers qui encouragent carrément la délation et qui permettent d'éviter des cas de fraude seulement par le biais de la création d'instruments de contrôle toujours plus envahissants par rapport aux personnes. Nous pouvons certainement - mon groupe et moi-même - soutenir avec conviction le rapport du président Napolitano. La modification du règlement et la proposition de décision répondent en effet sérieusement à une préoccupation qui a été soulevée récemment et que M. Dell'Alba a été le dernier à exprimer : ne pas soumettre les députés à des contrôles excessifs et arbitraires. Je ne vois aucune raison pour retarder davantage cette décision. Si un tel abus devait se produire, le Parlement européen et les députés disposent de larges possibilités pour se défendre. Des procédures internes plus transparentes et plus claires rendront évidemment les choses plus faciles. Nous devons avoir confiance dans le travail futur de l'OLAF et nous devons faciliter ses actions. Ce n'est que de cette manière que nous pourrons réagir de façon crédible en cas d'instrumentalisations. Ce n'est qu'ainsi que nous pourrons faire preuve d'efficacité lorsqu'il s'agira de répondre au genre de situations dont ont parlé aujourd'hui Mme Theato et M. Bösch.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b2151a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 0 / 500\n",
      "Numer of samples with no words found: 2 / 12917\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "# Get the sentence embeddings for the train dataset\n",
    "X_train = vectorizer.fit_transform(X_train_sample)\n",
    "y_train = y_train_sample\n",
    "# Get the sentence embeddings for the test dataset\n",
    "X_test = vectorizer.transform(X_test)\n",
    "#Y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff82aad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10fab4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create the model, train it, print scores\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"train score:\", clf.score(X_train, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7adc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df376ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder='.'\n",
    "# Global parameters\n",
    "#root folder\n",
    "root_folder='.'\n",
    "glove_filename='vectors.txt'\n",
    "\n",
    "# Variable for data directory\n",
    "glove_path = os.path.abspath(glove_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eca3a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Julie/OneDrive/Bureau/Apprentissage_Artificiel/vectors.txt\n"
     ]
    }
   ],
   "source": [
    "print(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91167792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188/2457025952.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file =glove_path\n",
    "tmp_file = \"test_word2vec.txt\"\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "model_bis = KeyedVectors.load_word2vec_format(tmp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ed37517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f1dd4f6fd30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32a1fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to Roi + Reine:  [('exercice,', 0.8002204895019531)]\n"
     ]
    }
   ],
   "source": [
    "result = model_bis.most_similar(positive=['Présidente', 'Président'], negative=['femme'], topn=1)\n",
    "\n",
    "print('Most similar word to Roi + Reine: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66a7750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vectorizer, ngram_range:tuple=(1,1)) -> None:\n",
    "        super().__init__()\n",
    "        self.vectorizer = vectorizer\n",
    "        self.ngram_range = ngram_range\n",
    "    def fit(self, X, y=None):\n",
    "        print(f\">>> Vectorizer.fit() called with vectorizer={self.vectorizer} and ngram_range={self.ngram_range}.\")\n",
    "        self.vectorizer.fit(X)\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        print(f\">>> Vectorizer.transform() called with vectorizer={self.vectorizer} and ngram_range={self.ngram_range}.\")\n",
    "        X_ = X.copy()\n",
    "        X_vect_ = self.vectorizer.transform(X_)\n",
    "        X_vect_ = X_vect_.toarray()\n",
    "        return X_vect_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee6f271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('président')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c3198da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = GloveVectorizer(model_bis)\n",
    "# Get the sentence embeddings for the train dataset\n",
    "#X_train = vectorizer.fit_transform(X_train_sample)\n",
    "#y_train = y_train_sample\n",
    "# Get the sentence embeddings for the test dataset\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "#Y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc7409ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GloveVectorizer at 0x7f1dc9d20730>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ab84deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of samples with no words found: 0 / 500\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(X_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c469d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01528353 -0.17387539 -0.04330047 ...  0.27416426 -0.94076514\n",
      "  -0.25462666]\n",
      " [-0.03737077 -0.02463203  0.14741205 ...  0.22106427 -0.7972185\n",
      "  -0.31570014]\n",
      " [ 0.0171264  -0.18690102 -0.07965815 ...  0.26617157 -0.91300488\n",
      "  -0.29518124]\n",
      " ...\n",
      " [-0.05366648 -0.19655336 -0.03869827 ...  0.31806138 -0.90246582\n",
      "  -0.30373943]\n",
      " [ 0.11446893 -0.12268434  0.00983311 ...  0.34728044 -0.79701775\n",
      "  -0.09281527]\n",
      " [-0.01101418 -0.15832914  0.01620665 ...  0.18718338 -0.85960853\n",
      "  -0.27101207]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc066e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
